{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common import exceptions\n",
    "import pymysql\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "import kss\n",
    "\n",
    "\n",
    "conn = pymysql.connect(host='newdb.c7p2ncpgik7h.ap-northeast-2.rds.amazonaws.com', user='admin', password='1dlckdals!',\n",
    "                       db='TEST1', charset='utf8')\n",
    "curs = conn.cursor()\n",
    "session = requests.Session()\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit 537.36 (KHTML, like Gecko) Chrome\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\"\n",
    "}\n",
    "\n",
    "\n",
    "chrome_options=webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "#driver = webdriver.Chrome(r\"/home/capston/chromedriver\",chrome_options=chrome_options)\n",
    "#driver = webdriver.Chrome(r\"C:\\Users\\LCM\\Downloads\\chromedriver_win32 (2)\\chromedriver.exe\")\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\LCM\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "#driver = webdriver.Chrome(r\"C:\\Users\\seenw\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "\n",
    "class SentenceTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.kkma = Kkma()\n",
    "        self.okt = Okt()\n",
    "        # 불용어 불러오기\n",
    "        self.stopwords = [line.rstrip('\\n') for line in open('stopwords_korean2.txt', encoding = 'utf-8')]\n",
    "    \n",
    "    def text2sentences(self, text):\n",
    "        #sentences = self.kkma.sentences(text)\\\n",
    "        sentences = kss.split_sentences(text)\n",
    "        sentences = sentences[0:len(sentences)-2]\n",
    "        \n",
    "        for idx in range(0, len(sentences)):\n",
    "            if len(sentences[idx]) <= 10:\n",
    "                sentences[idx-1] += (' ' + sentences[idx])\n",
    "                sentences[idx] = ''\n",
    "                \n",
    "        return sentences\n",
    "    \n",
    "    def get_nouns(self, sentences):\n",
    "        nouns = []\n",
    "        for sentence in sentences:\n",
    "            if sentence != '':\n",
    "                nouns.append(' '.join([noun for noun in self.okt.nouns(str(sentence))\n",
    "                    if noun not in self.stopwords and len(noun) > 1]))\n",
    "                \n",
    "        return nouns\n",
    "\n",
    "\n",
    "class GraphMatrix(object):\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer()\n",
    "        self.cnt_vec = CountVectorizer()\n",
    "        self.graph_sentence = []\n",
    "        \n",
    "    def build_sent_graph(self, sentence):\n",
    "        tfidf_mat = self.tfidf.fit_transform(sentence).toarray()\n",
    "        self.graph_sentence = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "        return self.graph_sentence\n",
    "    \n",
    "    def build_words_graph(self, sentence):\n",
    "        cnt_vec_mat = normalize(self.cnt_vec.fit_transform(sentence).toarray().astype(float), axis=0)\n",
    "        vocab = self.cnt_vec.vocabulary_\n",
    "        return np.dot(cnt_vec_mat.T, cnt_vec_mat), {vocab[word] : word for word in vocab}\n",
    "    \n",
    "    \n",
    "class Rank(object):\n",
    "    def get_ranks(self, graph, d=0.85): # d = damping factor\n",
    "        A = graph\n",
    "        matrix_size = A.shape[0]\n",
    "        for id in range(matrix_size):\n",
    "            A[id, id] = 0 # diagonal 부분을 0으로\n",
    "            link_sum = np.sum(A[:,id]) # A[:, id] = A[:][id]\n",
    "            if link_sum != 0:\n",
    "                A[:, id] /= link_sum\n",
    "                \n",
    "            A[:, id] *= -d\n",
    "            A[id, id] = 1\n",
    "            \n",
    "        B = (1-d) * np.ones((matrix_size, 1))\n",
    "        ranks = np.linalg.solve(A, B) # 연립방정식 Ax = b\n",
    "        return {idx: r[0] for idx, r in enumerate(ranks)}\n",
    "\n",
    "    \n",
    "    \n",
    "class TextRank(object):\n",
    "    def __init__(self, text):\n",
    "        self.sent_tokenize = SentenceTokenizer()\n",
    "        self.sentences = self.sent_tokenize.text2sentences(text)\n",
    "            \n",
    "        self.nouns = self.sent_tokenize.get_nouns(self.sentences)\n",
    "        self.graph_matrix = GraphMatrix()\n",
    "        self.sent_graph = self.graph_matrix.build_sent_graph(self.nouns)\n",
    "        self.words_graph, self.idx2word = self.graph_matrix.build_words_graph(self.nouns)\n",
    "        self.rank = Rank()\n",
    "        self.sent_rank_idx = self.rank.get_ranks(self.sent_graph)\n",
    "        self.sorted_sent_rank_idx = sorted(self.sent_rank_idx, key=lambda k: self.sent_rank_idx[k], reverse=True)\n",
    "        self.word_rank_idx = self.rank.get_ranks(self.words_graph)\n",
    "        self.sorted_word_rank_idx = sorted(self.word_rank_idx, key=lambda k: self.word_rank_idx[k], reverse=True)\n",
    "        #print(self.nouns)\n",
    "        \n",
    "    def summarize(self, sent_num=3):\n",
    "        summary = []\n",
    "        index=[]\n",
    "        for idx in self.sorted_sent_rank_idx[:sent_num]:\n",
    "            index.append(idx)\n",
    "        \n",
    "        index.sort()\n",
    "        for idx in index:\n",
    "            summary.append(self.sentences[idx])\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def keywords(self, word_num=10):\n",
    "        rank = Rank()\n",
    "        rank_idx = rank.get_ranks(self.words_graph)\n",
    "        sorted_rank_idx = sorted(rank_idx, key=lambda k: rank_idx[k], reverse=True)\n",
    "        \n",
    "        keywords = []\n",
    "        index=[]\n",
    "        for idx in sorted_rank_idx[:word_num]:\n",
    "            index.append(idx)\n",
    "            \n",
    "        #index.sort()\n",
    "        for idx in index:\n",
    "            keywords.append(self.idx2word[idx])\n",
    "            \n",
    "        return keywords\n",
    "\n",
    "\n",
    "class newsCrawlerNaver:\n",
    "    def __init__(self):\n",
    "        self.titleList=[]\n",
    "        self.contentsList=[]\n",
    "        self.imageList=[]\n",
    "        self.dateList=[]\n",
    "    # 네이버 뉴스홈\n",
    "    def mainCrawl(self):    \n",
    "        # 정치=100 경제=101 사회=102 생활/문화=103 세계=104 IT/과학=105\n",
    "        for category in range(100, 106):\n",
    "            main_url = \"https://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=\"+str(category)\n",
    "            driver.get(main_url)\n",
    "            \n",
    "            # '헤드라인 더보기' 버튼이 있다면 누르기       \n",
    "            self.showMore()\n",
    "            driver.implicitly_wait(3)\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')  \n",
    "            \n",
    "            # 헤드라인 가져오기\n",
    "            self.subCrawl(soup,category)\n",
    "        driver.quit()\n",
    "    \n",
    "    # 더보기버튼 클릭\n",
    "    def showMore(self):\n",
    "        try:\n",
    "            while True:\n",
    "                print(\"더보기\")\n",
    "                driver.find_element_by_xpath('//*[@id=\"main_content\"]/div/div[2]/div[2]/div/a').click()\n",
    "                driver.implicitly_wait(0.5)\n",
    "        except exceptions.ElementNotVisibleException:\n",
    "            pass\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "\n",
    "    # 헤드라인 뉴스 크롤링\n",
    "    def subCrawl(self, soup,category):\n",
    "        # 모든 헤드라인 뉴스 저장\n",
    "        articles = soup.find_all('div', {'class': 'cluster_group _cluster_content'})\n",
    "        \n",
    "        for i in range(len(articles)):\n",
    "            # 각 뉴스 본문에 있는 이미지와 이미지URL를 저장할 리스트\n",
    "            \n",
    "            company=\"\"\n",
    "            \n",
    "            images=[]\n",
    "            imagesURL=\"NO IMAGE\"\n",
    "\n",
    "            temp = articles[i].find_all('div', {'class': 'cluster_text'})[0]\n",
    "\n",
    "            conURL = temp.a.get('href')\n",
    "            html2 = session.get(conURL,headers=headers).content\n",
    "            soup2 = BeautifulSoup(html2, 'html.parser')\n",
    "            \n",
    "            company = soup2.find('meta', {'property':'me2:category1'}).get('content')\n",
    "            \n",
    "            summary = soup2.find('strong', {'class':'media_end_summary'})\n",
    "            if summary==None:\n",
    "                summary=\"\"\n",
    "            else:\n",
    "                summary=summary.text\n",
    "            \n",
    "            content = soup2.find('div', id= \"articleBodyContents\").text.replace(\"\\n\",\" \").replace(str(summary),\"\").replace(\"\\t\",\" \").replace(\"// flash 오류를 우회하기 위한 함수 추가 function _flash_removeCallback() {}\",\" \")\n",
    "            title=soup2.find('h3',id=\"articleTitle\").text\n",
    "            \n",
    "            # 기사 본문이 10문장이하라면 저장하지 않는다.\n",
    "            if(len(kss.split_sentences(content)) <= 10):\n",
    "                continue;\n",
    "\n",
    "            date=soup2.find('span', {'class','t11'}).text\n",
    "\n",
    "            images=soup2.find_all('span', {'class','end_photo_org'})\n",
    "            \n",
    "            for i in range(len(images)):\n",
    "                imagesURL=(images[i].find(\"img\")[\"src\"])\n",
    "            \n",
    "            self.saveToDB(str(title),str(content),str(imagesURL),str(date),str(category),str(company))\n",
    "\n",
    "\n",
    "        # DB에 저장\n",
    "        \n",
    "        #self.saveToDB(self.titleList,self.contentsList,self.imageList,self.dateList)\n",
    "\n",
    "    def saveToDB(self,title,content,imagesURL,date,category,company):\n",
    "        content=content.replace(\"'\",\"\")\n",
    "        sum = TextRank(content)\n",
    "        \n",
    "        content=sum.summarize(7)\n",
    "        count=1\n",
    "        for i in content:\n",
    "            if i==\"\":\n",
    "                print('중지됨')\n",
    "                return 3\n",
    "            print(i)\n",
    "            print(count)\n",
    "            print(\"\\n\")\n",
    "            count=count+1\n",
    "        if len(content)<7:\n",
    "            return 2\n",
    "        title = title.replace(\"'\",\"\")\n",
    "\n",
    "        # SQL문 실행\n",
    "        sql = \"USE TEST1\"\n",
    "        curs.execute(sql)\n",
    "        '''\n",
    "        CREATE TABLE NEWS8 (TITLE CHAR(200) NOT NULL,\n",
    "        CONTENT1 TEXT NOT NULL,\n",
    "        CONTENT2 TEXT NOT NULL,\n",
    "        CONTENT3 TEXT NOT NULL,\n",
    "        CONTENT4 TEXT NOT NULL,\n",
    "        CONTENT5 TEXT NOT NULL,\n",
    "        CONTENT6 TEXT NOT NULL,\n",
    "        CONTENT7 TEXT NOT NULL,\n",
    "        COMPANY VARCHAR(20) NOT NULL,\n",
    "        DATE VARCHAR(40) NOT NULL,\n",
    "        CATEGORY VARCHAR(40),\n",
    "        COUNT int NOT NULL AUTO_INCREMENT,\n",
    "        IMAGE TEXT NOT NULL,\n",
    "        CONSTRAINT PLAYER_PK PRIMARY KEY (COUNT));\n",
    "        '''\n",
    "\n",
    "        #sql3=\"insert into NEWS3(title,content,date,category,image) VALUES(\" +title+ ',' +content+ ',' +date+ ',' +category+ ',' +imagesURL+ \");\"\n",
    "        sql3=\"\"\"insert into NEWS8(title,content1,content2,content3,content4,content5,content6,content7,company,date,category,image) VALUES('%s','%s','%s','%s','%s','%s','%s','%s','%s','%s','%s','%s');\"\"\"%(title,content[0],content[1],content[2],content[3],content[4],content[5],content[6],company,date,category,imagesURL)\n",
    "        curs.execute(sql3)\n",
    "        conn.commit()\n",
    "        print(\"db updated!\")\n",
    "\n",
    "        return 1\n",
    "\n",
    "class newsCrawlerNate:\n",
    "    def __init__(self):\n",
    "        self.titleList=[]\n",
    "        self.contentsList=[]\n",
    "        self.imageList=[]\n",
    "        self.dateList=[]\n",
    "        \n",
    "    # 네이버 뉴스홈\n",
    "    def mainCrawl(self):    \n",
    "        \n",
    "        for category in range(200, 601,100):\n",
    "            main_url = \"https://news.nate.com/section?mid=n0\"+str(category)\n",
    "            driver.get(main_url)\n",
    "            \n",
    "            driver.implicitly_wait(0.1)\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')  \n",
    "            \n",
    "            # 헤드라인 가져오기\n",
    "            self.subCrawl(soup,category)\n",
    "        driver.quit()\n",
    "    \n",
    "\n",
    "    # 헤드라인 뉴스 크롤링\n",
    "    def subCrawl(self, soup, category):\n",
    "                # 모든 헤드라인 뉴스 저장\n",
    "        articles = soup.find_all('div', {'class': 'mlt01'})\n",
    "\n",
    "        for i in range(len(articles)):\n",
    "            # 각 뉴스 본문에 있는 이미지와 이미지URL를 저장할 리스트\n",
    "            \n",
    "            company=\"\"\n",
    "            \n",
    "            imagesURL=\"NO IMAGE\"\n",
    "\n",
    "            conURL = \"https:\" + articles[i].a.get('href')\n",
    "\n",
    "            html2 = session.get(conURL,headers=headers).content\n",
    "\n",
    "            soup2 = BeautifulSoup(html2, 'html.parser')\n",
    "\n",
    "            company = \"test\"\n",
    "            \n",
    "            imgSummary=soup2.find_all('span', {'class':'sub_tit'})#.text\n",
    "                        \n",
    "            summary = soup2.find('strong', {'class':'media_end_summary'})\n",
    "            if summary==None:\n",
    "                summary=\"\"\n",
    "            else:\n",
    "                summary=summary.text\n",
    "            \n",
    "            contentTemp=\"\"\n",
    "            try:\n",
    "                contentTemp=soup2.find('div', id= \"realArtcContents\").find('dl').getText()\n",
    "            except:\n",
    "                try:\n",
    "                    contentTemp=soup2.find('div', id= \"realArtcContents\").find('ul').getText()\n",
    "                except:\n",
    "                    \n",
    "                    pass\n",
    "                \n",
    "            contentTemp2=soup2.find('div', id= \"realArtcContents\").find('script').getText()\n",
    "            contentTemp3=soup2.find('div', id= \"realArtcContents\").find_all('a')\n",
    "            \n",
    "            content = soup2.find('div', id= \"realArtcContents\").getText()\n",
    "            \n",
    "            for i in range(len(imgSummary)):\n",
    "                content.replace(imgSummary[i].text,\"\")\n",
    "            for i in range(len(contentTemp3)):\n",
    "                content.replace(contentTemp3[i].text,\"\")\n",
    "                \n",
    "\n",
    "            content = content.replace(contentTemp,\" \").replace(contentTemp2,\" \").replace(\"\\n\",\" \").replace(\"\\t\",\" \")\n",
    "            title = soup2.find('meta', {'property':'og:title'}).get('content')\n",
    "            images=soup2.find('meta', {'property':'og:image'})\n",
    "            date=soup2.find('span', {'class','firstDate'}).find('em').getText()\n",
    "            if images!=None:\n",
    "                imagesURL=images.get('content')\n",
    "\n",
    "\n",
    "            # 기사 본문이 10문장이하라면 저장하지 않는다.\n",
    "            if(len(kss.split_sentences(content)) <= 10):\n",
    "                continue;\n",
    "\n",
    "\n",
    "            \n",
    "            self.saveToDB(str(title),str(content),str(imagesURL),str(date),str(category),str(company))\n",
    "\n",
    "\n",
    "    def saveToDB(self,title,content,imagesURL,date,category,company):\n",
    "        content=content.replace(\"'\",\"\")\n",
    "        sum = TextRank(content)\n",
    "        \n",
    "        content=sum.summarize(7)\n",
    "        count=1\n",
    "        for i in content:\n",
    "            if i==\"\":\n",
    "                print('중지됨')\n",
    "                return 3\n",
    "            print(i)\n",
    "            print(count)\n",
    "            print(\"\\n\")\n",
    "            count=count+1\n",
    "        if len(content)<7:\n",
    "            return 2\n",
    "        title = title.replace(\"'\",\"\")\n",
    "\n",
    "        # SQL문 실행\n",
    "        sql = \"USE TEST1\"\n",
    "        curs.execute(sql)\n",
    "        '''\n",
    "        CREATE TABLE NEWS8 (TITLE CHAR(200) NOT NULL,\n",
    "        CONTENT1 TEXT NOT NULL,\n",
    "        CONTENT2 TEXT NOT NULL,\n",
    "        CONTENT3 TEXT NOT NULL,\n",
    "        CONTENT4 TEXT NOT NULL,\n",
    "        CONTENT5 TEXT NOT NULL,\n",
    "        CONTENT6 TEXT NOT NULL,\n",
    "        CONTENT7 TEXT NOT NULL,\n",
    "        COMPANY VARCHAR(20) NOT NULL,\n",
    "        DATE VARCHAR(40) NOT NULL,\n",
    "        CATEGORY VARCHAR(40),\n",
    "        COUNT int NOT NULL AUTO_INCREMENT,\n",
    "        IMAGE TEXT NOT NULL,\n",
    "        CONSTRAINT PLAYER_PK PRIMARY KEY (COUNT));\n",
    "        '''\n",
    "\n",
    "        \n",
    "        sql3=\"\"\"insert into NEWS8(title,content1,content2,content3,content4,content5,content6,content7,company,date,category,image) VALUES('%s','%s','%s','%s','%s','%s','%s','%s','%s','%s','%s','%s');\"\"\"%(title,content[0],content[1],content[2],content[3],content[4],content[5],content[6],company,date,category,imagesURL)\n",
    "        curs.execute(sql3)\n",
    "        conn.commit()\n",
    "        print(\"db updated!\")\n",
    "\n",
    "        return 1\n",
    "\n",
    "\n",
    "crawlNaver=newsCrawlerNaver() \n",
    "crawlNaver.mainCrawl()\n",
    "crawlNate=newsCrawlerNate() \n",
    "crawlNate.mainCrawl()\n",
    "\n",
    "conn.commit()\n",
    "curs.close()\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
