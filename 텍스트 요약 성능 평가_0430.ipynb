{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common import exceptions\n",
    "import pymysql\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "import kss\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "\n",
    "def cosine_sim():\n",
    "    df =  pd.read_excel('news.xlsx')\n",
    "    # 모든 단어들의 빈도에 대하여 유사도를 계산하면 값이 너무 작게 나와서\n",
    "    # max_features옵션으로 Tf-Idf의 크기를 줄인 다음 코사인 유사도를 계산함.\n",
    "    tfidf = TfidfVectorizer(max_features=100)\n",
    "    \n",
    "    # doc: 기사 본문(문서)\n",
    "    # tfidf_mat: 문서들을 벡터화한 \n",
    "    doc = list(df['content'])\n",
    "    tfidf_mat = tfidf.fit_transform(doc).toarray()\n",
    "    #print(type(tfidf_mat))\n",
    "    print(tfidf_mat)\n",
    "    print(tfidf_mat.shape)\n",
    "    \n",
    "    # 소수점 4자리까지 반올림\n",
    "    sim = np.round(cosine_similarity(tfidf_mat, tfidf_mat),4)\n",
    "    print(sim)\n",
    "    print(type(sim))\n",
    "    print(sim.shape)\n",
    "    \n",
    "    print(sim)\n",
    "    \n",
    "    sim_scores = list(enumerate(sim[0]))\n",
    "    print(sim_scores)\n",
    "    \n",
    "    # 유사도가 높은 순서대로 정렬\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # 상위 인덱스와 유사도 추출\n",
    "    sim_scores = sim_scores[1:6]\n",
    "    print(sim_scores)\n",
    "    \n",
    "    # 첫 번째 기사와 유사한 기사 인덱스를 이용하여 제목 출력\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    print(df['title'].iloc[movie_indices])\n",
    "\n",
    "class SentenceTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.kkma = Kkma()\n",
    "        self.okt = Okt()\n",
    "        # 불용어 불러오기\n",
    "        self.stopwords = [line.rstrip('\\n') for line in open('stopwords_korean2.txt', encoding = 'utf-8')]\n",
    "    \n",
    "    def text2sentences(self, text):\n",
    "        #sentences = self.kkma.sentences(text)\\\n",
    "        sentences = kss.split_sentences(text)\n",
    "        sentences = sentences[0:len(sentences)-2]\n",
    "        \n",
    "        for idx in range(0, len(sentences)):\n",
    "            if len(sentences[idx]) <= 10:\n",
    "                sentences[idx-1] += (' ' + sentences[idx])\n",
    "                sentences[idx] = ''\n",
    "                \n",
    "        return sentences\n",
    "    \n",
    "    def get_nouns(self, sentences):\n",
    "        nouns = []\n",
    "        for sentence in sentences:\n",
    "            if sentence != '':\n",
    "                nouns.append(' '.join([noun for noun in self.okt.nouns(str(sentence))\n",
    "                    if noun not in self.stopwords and len(noun) > 1]))\n",
    "                \n",
    "        return nouns\n",
    "\n",
    "\n",
    "class GraphMatrix(object):\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer()\n",
    "        self.cnt_vec = CountVectorizer()\n",
    "        self.graph_sentence = []\n",
    "        \n",
    "    def build_sent_graph(self, sentence):\n",
    "        tfidf_mat = self.tfidf.fit_transform(sentence).toarray()\n",
    "        self.graph_sentence = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "        return self.graph_sentence\n",
    "    \n",
    "    def build_words_graph(self, sentence):\n",
    "        cnt_vec_mat = normalize(self.cnt_vec.fit_transform(sentence).toarray().astype(float), axis=0)\n",
    "        vocab = self.cnt_vec.vocabulary_\n",
    "        return np.dot(cnt_vec_mat.T, cnt_vec_mat), {vocab[word] : word for word in vocab}\n",
    "    \n",
    "    \n",
    "class Rank(object):\n",
    "    def get_ranks(self, graph, d=0.85): # d = damping factor\n",
    "        A = graph\n",
    "        matrix_size = A.shape[0]\n",
    "        for id in range(matrix_size):\n",
    "            A[id, id] = 0 # diagonal 부분을 0으로\n",
    "            link_sum = np.sum(A[:,id]) # A[:, id] = A[:][id]\n",
    "            if link_sum != 0:\n",
    "                A[:, id] /= link_sum\n",
    "                \n",
    "            A[:, id] *= -d\n",
    "            A[id, id] = 1\n",
    "            \n",
    "        B = (1-d) * np.ones((matrix_size, 1))\n",
    "        ranks = np.linalg.solve(A, B) # 연립방정식 Ax = b\n",
    "        return {idx: r[0] for idx, r in enumerate(ranks)}\n",
    "\n",
    "    \n",
    "    \n",
    "class TextRank(object):\n",
    "    def __init__(self, text):\n",
    "        self.sent_tokenize = SentenceTokenizer()\n",
    "        self.sentences = self.sent_tokenize.text2sentences(text)\n",
    "            \n",
    "        self.nouns = self.sent_tokenize.get_nouns(self.sentences)\n",
    "        self.graph_matrix = GraphMatrix()\n",
    "        self.sent_graph = self.graph_matrix.build_sent_graph(self.nouns)\n",
    "        self.words_graph, self.idx2word = self.graph_matrix.build_words_graph(self.nouns)\n",
    "        self.rank = Rank()\n",
    "        self.sent_rank_idx = self.rank.get_ranks(self.sent_graph)\n",
    "        self.sorted_sent_rank_idx = sorted(self.sent_rank_idx, key=lambda k: self.sent_rank_idx[k], reverse=True)\n",
    "        self.word_rank_idx = self.rank.get_ranks(self.words_graph)\n",
    "        self.sorted_word_rank_idx = sorted(self.word_rank_idx, key=lambda k: self.word_rank_idx[k], reverse=True)\n",
    "        #print(self.nouns)\n",
    "        \n",
    "    def summarize(self, sent_num=3):\n",
    "        summary = []\n",
    "        index=[]\n",
    "        for idx in self.sorted_sent_rank_idx[:sent_num]:\n",
    "            index.append(idx)\n",
    "        \n",
    "        index.sort()\n",
    "        for idx in index:\n",
    "            summary.append(self.sentences[idx])\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def keywords(self, word_num=10):\n",
    "        rank = Rank()\n",
    "        rank_idx = rank.get_ranks(self.words_graph)\n",
    "        sorted_rank_idx = sorted(rank_idx, key=lambda k: rank_idx[k], reverse=True)\n",
    "        \n",
    "        keywords = []\n",
    "        index=[]\n",
    "        for idx in sorted_rank_idx[:word_num]:\n",
    "            index.append(idx)\n",
    "            \n",
    "        #index.sort()\n",
    "        for idx in index:\n",
    "            keywords.append(self.idx2word[idx])\n",
    "            \n",
    "        return keywords\n",
    "\n",
    "\n",
    "class newsCrawlerNaver:\n",
    "    def __init__(self):\n",
    "        self.titleList=[]\n",
    "        self.contentsList=[]\n",
    "        self.imageList=[]\n",
    "        self.dateList=[]\n",
    "    # 네이버 뉴스홈\n",
    "    def mainCrawl(self):    \n",
    "        # 정치=100 경제=101 사회=102 생활/문화=103 세계=104 IT/과학=105\n",
    "        for category in range(100, 106):\n",
    "            main_url = \"https://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=\"+str(category)\n",
    "            driver.get(main_url)\n",
    "            \n",
    "            # '헤드라인 더보기' 버튼이 있다면 누르기       \n",
    "            self.showMore()\n",
    "            driver.implicitly_wait(0.5)\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')  \n",
    "            \n",
    "            # 헤드라인 가져오기\n",
    "            self.subCrawl(soup,category)\n",
    "        #driver.quit()\n",
    "    \n",
    "    # 더보기버튼 클릭\n",
    "    def showMore(self):\n",
    "        try:\n",
    "            while True:\n",
    "                print(\"더보기\")\n",
    "                driver.find_element_by_xpath('//*[@id=\"main_content\"]/div/div[2]/div[2]/div/a').click()\n",
    "                driver.implicitly_wait(0.5)\n",
    "        except exceptions.ElementNotVisibleException:\n",
    "            pass\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "\n",
    "    # 헤드라인 뉴스 크롤링\n",
    "    def subCrawl(self, soup,category):\n",
    "        # 모든 헤드라인 뉴스 저장\n",
    "        articles = soup.find_all('div', {'class': 'cluster_group _cluster_content'})\n",
    "        \n",
    "        for i in range(len(articles)):\n",
    "            # 각 뉴스 본문에 있는 이미지와 이미지URL를 저장할 리스트\n",
    "            \n",
    "            company=\"\"\n",
    "            \n",
    "            images=[]\n",
    "            imagesURL=\"NO IMAGE\"\n",
    "\n",
    "            temp = articles[i].find_all('div', {'class': 'cluster_text'})[0]\n",
    "\n",
    "            conURL = temp.a.get('href')\n",
    "            html2 = session.get(conURL,headers=headers).content\n",
    "            soup2 = BeautifulSoup(html2, 'html.parser')\n",
    "            \n",
    "            company = soup2.find('meta', {'property':'me2:category1'}).get('content')\n",
    "            \n",
    "            summary = soup2.find('strong', {'class':'media_end_summary'})\n",
    "            if summary==None:\n",
    "                summary=\"\"\n",
    "            else:\n",
    "                summary=summary.text\n",
    "            \n",
    "            content = soup2.find('div', id= \"articleBodyContents\").text.replace(\"\\n\",\" \").replace(str(summary),\"\").replace(\"\\t\",\" \").replace(\"// flash 오류를 우회하기 위한 함수 추가 function _flash_removeCallback() {}\",\" \")\n",
    "            title=soup2.find('h3',id=\"articleTitle\").text\n",
    "            \n",
    "            # 기사 본문이 10문장이하라면 저장하지 않는다.\n",
    "            if(len(kss.split_sentences(content)) <= 10):\n",
    "                continue;\n",
    "\n",
    "            date=soup2.find('span', {'class','t11'}).text\n",
    "\n",
    "            images=soup2.find_all('span', {'class','end_photo_org'})\n",
    "            \n",
    "            for i in range(len(images)):\n",
    "                imagesURL=(images[i].find(\"img\")[\"src\"])\n",
    "                \n",
    "            self.titleList.append(title)\n",
    "            self.contentsList.append(content)\n",
    "            self.dateList.append(date)\n",
    "            \n",
    "            self.saveToDB(str(title),str(content),str(imagesURL),str(date),str(category),str(company))\n",
    "        # 엑셀 파일 읽기\n",
    "        try:\n",
    "            news_df = pd.read_excel('news.xlsx')\n",
    "        except:\n",
    "            temp = pd.DataFrame(columns=('date','title','content','category'))\n",
    "            temp.set_index('date', inplace=True)\n",
    "            temp.to_excel('news.xlsx')\n",
    "            news_df = pd.read_excel('news.xlsx')\n",
    "            \n",
    "        news_df.set_index('date', inplace = True)\n",
    "    \n",
    "        # 크롤링한 기사 정보로 데이터프레임 생성\n",
    "        crawl_df = pd.DataFrame({'title':self.titleList, 'content':self.contentsList, 'date':self.dateList})\n",
    "        crawl_df['category'] = pd.Series([category for i in range(len(crawl_df.index))])\n",
    "        crawl_df.set_index('date', inplace = True)\n",
    "        \n",
    "        news_df = pd.concat([news_df, crawl_df])\n",
    "        \n",
    "        # 엑셀 파일 쓰기\n",
    "        news_df.to_excel('news.xlsx')            \n",
    "        self.titleList=[]\n",
    "        self.contentsList=[]\n",
    "        self.imageList=[]\n",
    "        self.dateList=[]\n",
    "\n",
    "\n",
    "    def saveToDB(self,title,content,imagesURL,date,category,company):\n",
    "        content=content.replace(\"'\",\"\")\n",
    "        sum = TextRank(content)\n",
    "        \n",
    "        content=sum.summarize(7)\n",
    "        count=1\n",
    "        for i in content:\n",
    "            if i==\"\":\n",
    "                print('중지됨')\n",
    "                return 3\n",
    "            print(i)\n",
    "            print(count)\n",
    "            print(\"\\n\")\n",
    "            count=count+1\n",
    "        if len(content)<7:\n",
    "            return 2\n",
    "        title = title.replace(\"'\",\"\")\n",
    "\n",
    "        # SQL문 실행\n",
    "        sql = \"USE TEST1\"\n",
    "        curs.execute(sql)\n",
    "        '''\n",
    "        CREATE TABLE NEWS8 (TITLE CHAR(200) NOT NULL,\n",
    "        CONTENT1 TEXT NOT NULL,\n",
    "        CONTENT2 TEXT NOT NULL,\n",
    "        CONTENT3 TEXT NOT NULL,\n",
    "        CONTENT4 TEXT NOT NULL,\n",
    "        CONTENT5 TEXT NOT NULL,\n",
    "        CONTENT6 TEXT NOT NULL,\n",
    "        CONTENT7 TEXT NOT NULL,\n",
    "        COMPANY VARCHAR(20) NOT NULL,\n",
    "        DATE VARCHAR(40) NOT NULL,\n",
    "        CATEGORY VARCHAR(40),\n",
    "        COUNT int NOT NULL AUTO_INCREMENT,\n",
    "        IMAGE TEXT NOT NULL,\n",
    "        CONSTRAINT PLAYER_PK PRIMARY KEY (COUNT));\n",
    "        '''\n",
    "\n",
    "        #sql3=\"insert into NEWS3(title,content,date,category,image) VALUES(\" +title+ ',' +content+ ',' +date+ ',' +category+ ',' +imagesURL+ \");\"\n",
    "        sql3=\"\"\"insert into NEWS10(title,content1,content2,content3,content4,content5,content6,content7,company,date,category,image) VALUES('%s','%s','%s','%s','%s','%s','%s','%s','%s','%s','%s','%s');\"\"\"%(title,content[0],content[1],content[2],content[3],content[4],content[5],content[6],company,date,category,imagesURL)\n",
    "        try:\n",
    "            curs.execute(sql3)\n",
    "        except:\n",
    "            pass\n",
    "        conn.commit()\n",
    "        print(\"db updated!\")\n",
    "\n",
    "        return 1\n",
    "\n",
    "class newsCrawlerNate:\n",
    "    def __init__(self):\n",
    "        self.titleList=[]\n",
    "        self.contentsList=[]\n",
    "        self.imageList=[]\n",
    "        self.dateList=[]\n",
    "        \n",
    "    # 네이버 뉴스홈\n",
    "    def mainCrawl(self):    \n",
    "        \n",
    "        for category in range(200, 601,100):\n",
    "            main_url = \"https://news.nate.com/section?mid=n0\"+str(category)\n",
    "            driver.get(main_url)\n",
    "            \n",
    "            driver.implicitly_wait(0.1)\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')  \n",
    "            \n",
    "            # 헤드라인 가져오기\n",
    "            self.subCrawl(soup,category)\n",
    "        driver.quit()\n",
    "    \n",
    "\n",
    "    # 헤드라인 뉴스 크롤링\n",
    "    def subCrawl(self, soup, category):\n",
    "                # 모든 헤드라인 뉴스 저장\n",
    "        articles = soup.find_all('div', {'class': 'mlt01'})\n",
    "\n",
    "        for i in range(len(articles)):\n",
    "            # 각 뉴스 본문에 있는 이미지와 이미지URL를 저장할 리스트\n",
    "            \n",
    "            company=\"\"\n",
    "            \n",
    "            imagesURL=\"NO IMAGE\"\n",
    "\n",
    "            conURL = \"https:\" + articles[i].a.get('href')\n",
    "\n",
    "            html2 = session.get(conURL,headers=headers).content\n",
    "\n",
    "            soup2 = BeautifulSoup(html2, 'html.parser')\n",
    "\n",
    "            company = \"test\"\n",
    "            \n",
    "            imgSummary=soup2.find_all('span', {'class':'sub_tit'})#.text\n",
    "                        \n",
    "            summary = soup2.find('strong', {'class':'media_end_summary'})\n",
    "            if summary==None:\n",
    "                summary=\"\"\n",
    "            else:\n",
    "                summary=summary.text\n",
    "            \n",
    "            contentTemp=\"\"\n",
    "            try:\n",
    "                contentTemp=soup2.find('div', id= \"realArtcContents\").find('dl').getText()\n",
    "            except:\n",
    "                try:\n",
    "                    contentTemp=soup2.find('div', id= \"realArtcContents\").find('ul').getText()\n",
    "                except:\n",
    "                    \n",
    "                    pass\n",
    "                \n",
    "            contentTemp2=soup2.find('div', id= \"realArtcContents\").find('script').getText()\n",
    "            contentTemp3=soup2.find('div', id= \"realArtcContents\").find_all('a')\n",
    "            \n",
    "            content = soup2.find('div', id= \"realArtcContents\").getText()\n",
    "            \n",
    "            for i in range(len(imgSummary)):\n",
    "                content.replace(imgSummary[i].text,\"\")\n",
    "            for i in range(len(contentTemp3)):\n",
    "                content.replace(contentTemp3[i].text,\"\")\n",
    "                \n",
    "\n",
    "            content = content.replace(contentTemp,\" \").replace(contentTemp2,\" \").replace(\"\\n\",\" \").replace(\"\\t\",\" \")\n",
    "            title = soup2.find('meta', {'property':'og:title'}).get('content')\n",
    "            images=soup2.find('meta', {'property':'og:image'})\n",
    "            date=soup2.find('span', {'class','firstDate'}).find('em').getText()\n",
    "            if images!=None:\n",
    "                imagesURL=images.get('content')\n",
    "\n",
    "\n",
    "            # 기사 본문이 10문장이하라면 저장하지 않는다.\n",
    "            if(len(kss.split_sentences(content)) <= 10):\n",
    "                continue;\n",
    "                \n",
    "            self.titleList.append(title)\n",
    "            self.contentsList.append(content)\n",
    "            self.dateList.append(date)\n",
    "\n",
    "            self.saveToDB(str(title),str(content),str(imagesURL),str(date),str(category),str(company))\n",
    "\n",
    "        # 엑셀 파일 읽기\n",
    "        try:\n",
    "            news_df = pd.read_excel('news.xlsx')\n",
    "        except:\n",
    "            temp = pd.DataFrame(columns=('date','title','content','category'))\n",
    "            temp.set_index('date', inplace=True)\n",
    "            temp.to_excel('news.xlsx')\n",
    "            news_df = pd.read_excel('news.xlsx')\n",
    "        news_df.set_index('date', inplace = True)\n",
    "    \n",
    "        # 크롤링한 기사 정보로 데이터프레임 생성\n",
    "        crawl_df = pd.DataFrame({'title':self.titleList, 'content':self.contentsList, 'date':self.dateList})\n",
    "        crawl_df['category'] = pd.Series([category for i in range(len(crawl_df.index))])\n",
    "        crawl_df.set_index('date', inplace = True)\n",
    "        \n",
    "        news_df = pd.concat([news_df, crawl_df])\n",
    "        \n",
    "        # 엑셀 파일 쓰기\n",
    "        news_df.to_excel('news.xlsx')            \n",
    "        self.titleList=[]\n",
    "        self.contentsList=[]\n",
    "        self.imageList=[]\n",
    "        self.dateList=[]\n",
    "            \n",
    "            \n",
    "            \n",
    "    def saveToDB(self,title,content,imagesURL,date,category,company):\n",
    "        content=content.replace(\"'\",\"\")\n",
    "        sum = TextRank(content)\n",
    "        \n",
    "        content=sum.summarize(7)\n",
    "        count=1\n",
    "        for i in content:\n",
    "            if i==\"\":\n",
    "                print('중지됨')\n",
    "                return 3\n",
    "            print(i)\n",
    "            print(count)\n",
    "            print(\"\\n\")\n",
    "            count=count+1\n",
    "        if len(content)<7:\n",
    "            return 2\n",
    "        title = title.replace(\"'\",\"\")\n",
    "\n",
    "        # SQL문 실행\n",
    "        sql = \"USE TEST1\"\n",
    "        curs.execute(sql)\n",
    "        '''\n",
    "        CREATE TABLE NEWS8 (TITLE CHAR(200) NOT NULL,\n",
    "        CONTENT1 TEXT NOT NULL,\n",
    "        CONTENT2 TEXT NOT NULL,\n",
    "        CONTENT3 TEXT NOT NULL,\n",
    "        CONTENT4 TEXT NOT NULL,\n",
    "        CONTENT5 TEXT NOT NULL,\n",
    "        CONTENT6 TEXT NOT NULL,\n",
    "        CONTENT7 TEXT NOT NULL,\n",
    "        COMPANY VARCHAR(20) NOT NULL,\n",
    "        DATE VARCHAR(40) NOT NULL,\n",
    "        CATEGORY VARCHAR(40),\n",
    "        COUNT int NOT NULL AUTO_INCREMENT,\n",
    "        IMAGE TEXT NOT NULL,\n",
    "        CONSTRAINT PLAYER_PK PRIMARY KEY (COUNT));\n",
    "        '''\n",
    "\n",
    "        \n",
    "        sql3=\"\"\"insert into NEWS10(title,content1,content2,content3,content4,content5,content6,content7,company,date,category,image) VALUES('%s','%s','%s','%s','%s','%s','%s','%s','%s','%s','%s','%s');\"\"\"%(title,content[0],content[1],content[2],content[3],content[4],content[5],content[6],company,date,category,imagesURL)\n",
    "        try:\n",
    "            curs.execute(sql3)\n",
    "        except:\n",
    "            pass\n",
    "        conn.commit()\n",
    "        print(\"db updated!\")\n",
    "\n",
    "        return 1\n",
    "    \n",
    "    \n",
    "    \n",
    "conn = pymysql.connect(host='newdb.c7p2ncpgik7h.ap-northeast-2.rds.amazonaws.com', user='admin', password='1dlckdals!',\n",
    "                       db='TEST1', charset='utf8')\n",
    "curs = conn.cursor()\n",
    "session = requests.Session()\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit 537.36 (KHTML, like Gecko) Chrome\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\"\n",
    "}\n",
    "\n",
    "\n",
    "# chrome_options=webdriver.ChromeOptions()\n",
    "# chrome_options.add_argument('--headless')\n",
    "# chrome_options.add_argument('--no-sandbox')\n",
    "# chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "#driver = webdriver.Chrome(r\"/home/capston/chromedriver\",chrome_options=chrome_options)\n",
    "#driver = webdriver.Chrome(r\"C:\\Users\\LCM\\Downloads\\chromedriver_win32 (2)\\chromedriver.exe\")\n",
    "#driver = webdriver.Chrome(r\"C:\\Users\\LCM\\Downloads\\chromedriver_win32 (4)\\chromedriver.exe\")\n",
    "#driver = webdriver.Chrome(r\"C:\\Users\\seenw\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "# crawlNaver=newsCrawlerNaver() \n",
    "# crawlNaver.mainCrawl()\n",
    "# crawlNate=newsCrawlerNate() \n",
    "# crawlNate.mainCrawl()\n",
    "\n",
    "\n",
    "# cosine_sim()\n",
    "\n",
    "# conn.commit()\n",
    "# curs.close()\n",
    "# print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=101&oid=023&aid=0003611305\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"b4ba47f677caa4c468396f899617a4a4\", element=\"e455a67c-17af-411a-8a1c-902b77f491bb\")>\n",
      "\n",
      "['']\n",
      "[]\n",
      "https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=101&oid=448&aid=0000326678\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common import exceptions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import kss\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "\n",
    "class newsCrawler:\n",
    "    def __init__(self):\n",
    "        self.titleList=[]\n",
    "        self.contentsList=[]\n",
    "        self.imageList=[]\n",
    "        self.dateList=[]\n",
    "        \n",
    "    # 네이버 뉴스홈\n",
    "    def mainCrawl(self):    \n",
    "        # 정치=100 경제=101 사회=102 생활/문화=103 세계=104 IT/과학=105\n",
    "        # 실제 실행에서는 range(100, 106)으로 해야함.\n",
    "        for category in range(101, 102):\n",
    "            main_url = \"https://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=\"+str(category)\n",
    "            driver.get(main_url)\n",
    "            \n",
    "            # '헤드라인 더보기' 버튼이 있다면 누르기       \n",
    "            self.showMore()\n",
    "            driver.implicitly_wait(5)\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')  \n",
    "            \n",
    "            # 헤드라인 가져오기\n",
    "            self.subCrawl(soup, category)\n",
    "        driver.quit()\n",
    "    \n",
    "    # 더보기버튼 클릭\n",
    "    def showMore(self):\n",
    "        try:\n",
    "            while True:\n",
    "                driver.find_element_by_xpath('//*[@id=\"main_content\"]/div/div[2]/div[2]/div/a').click()\n",
    "        except exceptions.ElementNotVisibleException as e:\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    # 헤드라인 뉴스 크롤링\n",
    "    def subCrawl(self, soup, category):\n",
    "        # 모든 헤드라인 뉴스 저장\n",
    "        articles = soup.find_all('div', {'class': 'cluster_group _cluster_content'})\n",
    "        sum_bots=[]\n",
    "        sum1=[]\n",
    "        sum2=[]\n",
    "        sum3=[]\n",
    "        \n",
    "        for i in range(len(articles)):\n",
    "            # 각 뉴스 본문에 있는 이미지와 이미지URL를 저장할 리스트\n",
    "            images=[]\n",
    "            imagesURL=[]\n",
    "            \n",
    "            # 본문 링크\n",
    "            temp = articles[i].find_all('div', {'class': 'cluster_text'})[0]\n",
    "            #print(temp.a.text)         \n",
    "            conURL = temp.a.get('href')\n",
    "            print(conURL)\n",
    "            \n",
    "            # 요약봇 3문장 크롤링\n",
    "            driver.get(conURL)\n",
    "            sum_bot=\"\"\n",
    "            \n",
    "            try:\n",
    "#                 while sum_bot==\"\":\n",
    "#                     driver.find_element_by_xpath('//*[@id=\"main_content\"]/div[1]/div[3]/div/div[3]/div[2]/div[1]/a').click()\n",
    "#                     driver.implicitly_wait(30)\n",
    "#                     sum_bot = driver.find_element_by_class_name(\"_contents_body\").text\n",
    "                driver.find_element_by_xpath('//*[@id=\"main_content\"]/div[1]/div[3]/div/div[3]/div[2]/div[1]/a').click()\n",
    "                element = WebDriverWait(driver, 10000).until(\n",
    "                    EC.presence_of_element_located((By.CLASS_NAME, \"_contents_body\"))\n",
    "                )\n",
    "                print(element)\n",
    "                sum_bot = element.text\n",
    "                print(sum_bot)\n",
    "                \n",
    "                print(sum_bot.split('\\n\\n'))\n",
    "                if len(sum_bot.split('\\n\\n'))<3:\n",
    "                    pass\n",
    "                else:\n",
    "                    sum1.append(sum_bot.split('\\n\\n')[0])\n",
    "                    sum2.append(sum_bot.split('\\n\\n')[1])\n",
    "                    sum3.append(sum_bot.split('\\n\\n')[2])\n",
    "                print(sum1)\n",
    "            except exceptions.ElementNotVisibleException as e:\n",
    "                pass\n",
    "            except Exception as e:\n",
    "                pass\n",
    "          \n",
    "            # 기사 제목, 내용 크롤링\n",
    "            html2 = session.get(conURL,headers=headers).content\n",
    "            soup2 = BeautifulSoup(html2, 'html.parser')\n",
    "            \n",
    "            summary = soup2.find('strong', {'class':'media_end_summary'})\n",
    "            if summary==None:\n",
    "                summary=\"\"\n",
    "            else:\n",
    "                summary=summary.text\n",
    "            content = soup2.find('div', id= \"articleBodyContents\").text.replace(\"\\n\",\" \").replace(str(summary),\"\").replace(\"\\t\",\" \").replace(\"// flash 오류를 우회하기 위한 함수 추가 function _flash_removeCallback() {}\",\" \")\n",
    "            title=soup2.find('h3',id=\"articleTitle\").text\n",
    "            \n",
    "            #print(content)\n",
    "            sum_3 = TextRank(content)\n",
    "            sum_3=sum_3.summarize(3)\n",
    "            #print(sum_3)\n",
    "            \n",
    "        df2 = pd.DataFrame({'sum1':sum1, 'sum2':sum2, 'sum3':sum3})\n",
    "        print(df)\n",
    "        df2.to_excel('summary_bot.xlsx')\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    def saveNewsToFile(self):\n",
    "        f = open('result0319.txt', 'w', encoding='utf-8')\n",
    "        for i in range(len(self.titleList)):\n",
    "            f.write(self.titleList[i] + \"\\n\")\n",
    "            f.writelines(self.contentsList[i]+\"\\n\")\n",
    "            f.writelines(self.imageList[i])\n",
    "            f.writelines(\"\\n\\n\")\n",
    "        f.close()\n",
    "    \n",
    "def cosine_sim(count):\n",
    "    df =  pd.read_excel('news.xlsx')\n",
    "    df = df.drop_duplicates(['title'], keep='first', ignore_index=True)\n",
    "    # 모든 단어들의 빈도에 대하여 유사도를 계산하면 값이 너무 작게 나와서\n",
    "    # max_features옵션으로 Tf-Idf의 크기를 줄인 다음 코사인 유사도를 계산함.\n",
    "    tfidf = TfidfVectorizer(max_features=100)\n",
    "    \n",
    "    # max_feature는 tf-idf vector의 최대 feature를 설정해주는 파라미터입니다.\n",
    "    # 머신러닝에서 feature란, 테이블의 컬럼에 해당하는 개념입니다. 또한 행렬의 열에 해당하는 것이기도 합니다.\n",
    "    # TF-IDF 벡터는 단어사전의 인덱스만큼 feature를 부여받습니다.\n",
    "    \n",
    "    # doc: 기사 본문(문서)\n",
    "    # tfidf_mat: 문서들을 벡터화한 \n",
    "    doc = list(df['content'])\n",
    "    tfidf_mat = tfidf.fit_transform(doc).toarray()\n",
    "    #print(type(tfidf_mat))\n",
    "    #print(tfidf_mat)\n",
    "    #print(tfidf_mat.shape)\n",
    "    \n",
    "    # 소수점 4자리까지 반올림\n",
    "    sim = np.round(cosine_similarity(tfidf_mat, tfidf_mat),4)\n",
    "    #print(sim)\n",
    "    #print(type(sim))\n",
    "    #print(sim.shape)\n",
    "    \n",
    "    #print(sim)\n",
    "    \n",
    "    sim_scores = list(enumerate(sim[count]))\n",
    "    #print(sim_scores)\n",
    "    \n",
    "    # 유사도가 높은 순서대로 정렬\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # 상위 인덱스와 유사도 추출\n",
    "    sim_scores = sim_scores[1:6]\n",
    "    #print(sim_scores)\n",
    "    \n",
    "    # 원하는 기사와 유사한 기사 인덱스를 이용하여 제목 출력\n",
    "    news_indices = [i[0] for i in sim_scores]\n",
    "    result = df['title'].iloc[news_indices]\n",
    "    print(result)\n",
    "    print(tuple(result.index))\n",
    "    \n",
    "    df.set_index('date', inplace = True)\n",
    "    df.to_excel('news.xlsx')\n",
    "    \n",
    "    return tuple(result.index)\n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit 537.36 (KHTML, like Gecko) Chrome\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\"\n",
    "}\n",
    "\n",
    "#driver = webdriver.Chrome(r\"C:\\Users\\LCM\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\seenw\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "c=newsCrawler()\n",
    "c.mainCrawl()\n",
    "#cosine_sim(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
